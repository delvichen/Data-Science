{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam machine learning terdapat bidang yang secara spesifik membahas dataset text.bidang ini dikenal sebagai Natural Language Processing atau biasa disingkat sebagai NLP.\n",
    "\n",
    "Dalam machine learning sendiri terdapat beberapa teknik yang umum digunakan untuk feature extraction dari dataset text.pada sesi kali ini,kita akan mempelajari dua diantaranya yaitu Bag of Words dan Stop Word Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words model sebagai representasi text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words menyederhanakan representasi text sebagai sekumpulan kata serta mengabaikan grammar dan posisi tiap kata pada kalimat. Text akan dikonversi menjadi lowercase dan tanda baca akan diabaikan.\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Bag-of-words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',\n",
    "    'Linux distributions include the Linux kernel.',\n",
    "    'Linux is one of the most prominent open-source software.'\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kasus kita kali ini datasetnya berupa sekumpulan kalimat pendek.dataset text ini juga sering dikenal dengan istilah \"Corpus\".Corpusnya terdiri dari tiga kalimat pendek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words model dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words model dapat diterapkan dengan memanfatkan CountVectorizer.\n",
    "\n",
    "*Kita akan memanfaatkan bag of words untuk melakukan feature extraction dari dataset yang kita miliki*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #import modul\n",
    "\n",
    "vectorizer = CountVectorizer() #membentuk objek\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense() #menerapkan fit transform \n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method \"todense\" akan mengkonversikan hasil fit transform dari objek vectorizer menjadi suatu array dua dimensi\n",
    "\n",
    "Setiap baris dari hasil tersebut menrepresentasikan tiap kalimat yang berada dalam corpus kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'of',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sewaktu kita memanggil \"vectorizer.get_feature_names\" maka akan dikembalikan sekumpulan kata yang berada dalam bag/keranjang\n",
    "* Semua case menjadi lowercase artinya sudah tidak ada lagi huruf besar\n",
    "* Setiap kata yang ditampung dalam bag atau keranjang ini juga dikenal dengan istilah token\n",
    "* Setiap nilai tersebut menrepresentasikan jumlah kemunculan token/kata tertentu pada kalimat\n",
    "\n",
    "Dari hasil kita mengetahui bahwa :\n",
    "* index ke-0 menrepresentasikan 1990s\n",
    "\n",
    "* index pertama menrepsentasikan around\n",
    "\n",
    "dst\n",
    "\n",
    "* Jika pada kalimat terdapat kata \"1990s\" maka hasil dari vectorizer adalah \"1\"\n",
    "* Jika pada kalimat tidak terdapat kata distributions maka hasil dari vectorizer adalah \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance untuk mengukur kedekatan/jarak antar dokumen (vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagaimana representasi text dalam format bag of words ini dapat membantu proses training dari suatu model atau algoritma machine learning? Jawabannya adalah dengan representasi bag of words,suatu algoritma machine learning dapat dengan lebih mudah mengukur kedekatan atau kemiripan antar dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2: [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3: [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3: [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances #import modul\n",
    "\n",
    "for i in range(len(vectorized_X)): #ukur jarak antarkalimat\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i], vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}: {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat dilihat dari hasil tersebut,nilai terkecil adalah yang baris pertama (Jarak dokumen 1 dan 2).kita bisa simpulkan bahwa tingkat kemiripan antara dokumen pertama dengan dokumen kedua itu yang paling tinggi diantara ketiga dokumen tersebut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Filtering pada text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Word Filtering menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an), auxiliary verbs (do, be, will), dan prepositions (on, in, at).\n",
    "\n",
    "Stop Word Filtering cukup umum ditemui dalam bidang NLP atau Natural Language Processing\n",
    "\n",
    "Referensi: https://en.wikipedia.org/wiki/Stop_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil tersebut,terdapat beberapa stop words salah satunya adalah the,has,been,is,of dan semuanya merupakan stop words yang akan kita abaikan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Filtering dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Word Filtering juga dapat diterapkan dengan memanfatkan CountVectorizer.\n",
    "\n",
    "*Memanfaatkan Stop Word Filtering untuk mengeluarkan stop words dari corpus yang kita miliki*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #import modul\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english') \n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense() #fit transform\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyertakan parameter stop_words yang kita beri nilai \"english\",karena kasusnya disini adalah melakukan Stop Word Filtering untuk bahasa inggris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil tersebut,merupakan kumpulan kata/tokens yang sudah kita filtering stop wordsnya.Alhasil kita akan memperoleh representasi dari suatu kalimat yang lebih sederhana"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
